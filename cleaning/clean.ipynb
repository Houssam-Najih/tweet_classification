{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e20553ed",
   "metadata": {},
   "source": [
    "# Nettoyage de tweets pour LLM\n",
    "\n",
    "L'objectif de ce notebook est de préparer un corpus de tweets pour un usage avec un modèle de langage (LLM).\n",
    "\n",
    "On va :\n",
    "\n",
    "1. Charger un fichier CSV d'export Twitter.\n",
    "2. Harmoniser les noms de colonnes (id, texte, etc.).\n",
    "3. Séparer les tweets :\n",
    "   - des **comptes Free** (annonces, réponses, retweets de Free),\n",
    "   - des **clients** (utilisateurs finaux).\n",
    "4. Nettoyer le texte des tweets clients :\n",
    "   - suppression des URLs ;\n",
    "   - normalisation des sauts de ligne (`\\n`, `\\\\n`, `\\r`, `\\t`) en espaces ;\n",
    "   - conservation des hashtags ;\n",
    "   - passage en minuscules.\n",
    "5. Dédupliquer les tweets sur l’identifiant `id`.\n",
    "6. Produire des statistiques de contrôle (nombre de tweets, retweets Free, hashtags perdus…).\n",
    "\n",
    "À la fin, on obtient un fichier CSV propre contenant uniquement des tweets clients nettoyés, prêt pour le LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec9aa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba46506",
   "metadata": {},
   "source": [
    "## 1. Fonctions utilitaires (regex, normalisation, hashtags, noms de colonnes)\n",
    "\n",
    "Dans cette partie, on définit :\n",
    "\n",
    "- des **expressions régulières** pour détecter :\n",
    "  - les URLs ;\n",
    "  - les retweets (tweets commençant par `RT`).\n",
    "- une fonction `normalize_whitespace_tokens` qui transforme toutes les variantes de sauts de ligne et tabulations (`\\n`, `\\\\n`, `\\r`, `\\\\r`, `\\t`, `\\\\t`) en espaces ;\n",
    "- une fonction `extract_hashtags` qui extrait la liste des hashtags à partir d'un texte **déjà normalisé** ;\n",
    "- une fonction `_normalize_col_name` qui homogénéise les noms de colonnes (minuscules, underscores, suppression de caractères cachés)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2bf182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex globales\n",
    "\n",
    "URL_PAT = re.compile(r'https?://\\S+|www\\.\\S+', flags=re.IGNORECASE)\n",
    "RT_PAT  = re.compile(r'^\\s*rt\\b', flags=re.IGNORECASE)  # RT au début de texte\n",
    "\n",
    "# Utils texte\n",
    "\n",
    "def normalize_whitespace_tokens(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalise les sauts de ligne / tabulations :\n",
    "    - remplace les séquences texte '\\\\n', '\\\\r', '\\\\t'\n",
    "    - ET les vrais caractères '\\n', '\\r', '\\t'\n",
    "    par des espaces.\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "\n",
    "    # Séquences échappées (backslash + lettre)\n",
    "    s = s.replace(\"\\\\r\", \" \").replace(\"\\\\n\", \" \").replace(\"\\\\t\", \" \")\n",
    "    # Caractères réels de contrôle\n",
    "    s = s.replace(\"\\r\", \" \").replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    return s\n",
    "\n",
    "\n",
    "def extract_hashtags(s_norm: str):\n",
    "    \"\"\"\n",
    "    Extrait les hashtags d'une chaîne DEJA normalisée\n",
    "    (pas de \\\\n / \\\\t bizarres).\n",
    "    Retourne une liste de tags commençant par '#'.\n",
    "    \"\"\"\n",
    "    if not isinstance(s_norm, str):\n",
    "        return []\n",
    "    return re.findall(r'#[^\\s#]+', s_norm, flags=re.UNICODE)\n",
    "\n",
    "\n",
    "def _normalize_col_name(c) -> str:\n",
    "    \"\"\"\n",
    "    Normalise les noms de colonnes :\n",
    "    - supprime le BOM éventuel\n",
    "    - strip\n",
    "    - lower\n",
    "    - remplace les espaces par des underscores\n",
    "    \"\"\"\n",
    "    return (\n",
    "        str(c)\n",
    "        .replace('\\ufeff', '')  # BOM éventuel\n",
    "        .strip()\n",
    "        .lower()\n",
    "        .replace(\" \", \"_\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e7f461",
   "metadata": {},
   "source": [
    "## 2. Fonction de nettoyage du texte pour le LLM\n",
    "\n",
    "La fonction `clean_llm_text` prend un texte **déjà normalisé** (sans `\\n` bizarres) et :\n",
    "\n",
    "1. Décode les entités HTML (`&amp;`, `&quot;`, etc.).\n",
    "2. Supprime les URLs.\n",
    "3. Normalise les espaces (un seul espace entre les mots).\n",
    "4. Vérifie que les hashtags présents dans le texte normalisé initial n'ont pas été perdus\n",
    "   et réinjecte ceux qui manquent à la fin du texte.\n",
    "5. Passe tout en minuscules (`casefold`).\n",
    "\n",
    "Le résultat est stocké dans la colonne `text_clean_llm`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4eccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_llm_text(text_norm: str) -> str:\n",
    "    \"\"\"\n",
    "    Nettoie un texte de tweet DEJA normalisé (text_norm) pour usage LLM :\n",
    "    1. Décodage HTML (&amp;, &quot;, etc.).\n",
    "    2. Suppression des URLs.\n",
    "    3. Normalisation des espaces.\n",
    "    4. Vérification / réinjection des hashtags perdus.\n",
    "    5. Passage en minuscules (casefold).\n",
    "    \"\"\"\n",
    "    if not isinstance(text_norm, str):\n",
    "        text_norm = str(text_norm) if text_norm is not None else \"\"\n",
    "\n",
    "    # Décodage HTML (&amp; -> &, etc.)\n",
    "    cleaned = html.unescape(text_norm)\n",
    "\n",
    "    # Suppression des URLs\n",
    "    cleaned = URL_PAT.sub(' ', cleaned)\n",
    "\n",
    "    # Normalisation des espaces (un seul espace entre les mots)\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "\n",
    "    # Gestion des hashtags possiblement perdus\n",
    "    raw_tags     = extract_hashtags(text_norm)   # hashtags sur texte brut normalisé\n",
    "    cleaned_tags = extract_hashtags(cleaned)     # hashtags sur texte nettoyé\n",
    "\n",
    "    cleaned_lower = {t.casefold() for t in cleaned_tags}\n",
    "    missing = [t for t in raw_tags if t.casefold() not in cleaned_lower]\n",
    "\n",
    "    if missing:\n",
    "        if cleaned and not cleaned.endswith(' '):\n",
    "            cleaned += ' '\n",
    "        cleaned += ' '.join(missing)\n",
    "        cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "\n",
    "    # Minuscule robuste\n",
    "    return cleaned.casefold()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b03027",
   "metadata": {},
   "source": [
    "## 3. Lecture robuste du fichier CSV\n",
    "\n",
    "La fonction `read_csv_robust` permet de charger le fichier CSV d'export Twitter :\n",
    "\n",
    "- essaie d'abord le format `UTF-8` ;\n",
    "- en cas d'échec, réessaie en `UTF-8-SIG` (fréquent pour les exports Excel) ;\n",
    "- laisse `pandas` détecter automatiquement le séparateur ;\n",
    "- force toutes les colonnes en chaînes (`dtype=str`).\n",
    "\n",
    "On obtient un `DataFrame` brut qui sera ensuite nettoyé.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae79cccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_robust(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Essaie UTF-8 puis UTF-8-SIG (Excel) avec détection automatique du séparateur.\n",
    "    Tout est lu en string (dtype=str).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(\n",
    "            path,\n",
    "            sep=None,\n",
    "            engine=\"python\",\n",
    "            encoding=\"utf-8\",\n",
    "            on_bad_lines=\"skip\",\n",
    "            dtype=str,\n",
    "        )\n",
    "    except Exception:\n",
    "        return pd.read_csv(\n",
    "            path,\n",
    "            sep=None,\n",
    "            engine=\"python\",\n",
    "            encoding=\"utf-8-sig\",\n",
    "            on_bad_lines=\"skip\",\n",
    "            dtype=str,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d2e0c8",
   "metadata": {},
   "source": [
    "## 4. Pipeline global de nettoyage des tweets\n",
    "\n",
    "La fonction `clean_dataframe(df)` applique tout le pipeline :\n",
    "\n",
    "1. Normalisation des noms de colonnes (minuscules, underscore, suppression BOM).\n",
    "2. Vérification de la présence de la colonne `id` (utilisée pour dédupliquer).\n",
    "3. Détection de la colonne texte (`full_text`, `text`, `tweet` ou `content`) et copie dans `text_raw`.\n",
    "4. Normalisation de `text_raw` avec `normalize_whitespace_tokens` (tous les `\\n`, `\\\\n`, `\\r`, `\\\\r`, `\\t`, `\\\\t` → espaces).\n",
    "5. Détection des retweets sur tout le dataset (`is_retweet`) :\n",
    "   - texte commençant par `RT` ;\n",
    "   - ou `retweeted_status` non nul si la colonne existe.\n",
    "6. Filtre des comptes Free (`free`, `free_1337`, `freebox`, `freefoot`, `freenewsactu`, `universfreebox`) :\n",
    "   - comptage des tweets Free filtrés (`reste`) ;\n",
    "   - comptage des retweets Free (`retweets_free`) ;\n",
    "   - conservation uniquement des tweets **clients** dans `df`.\n",
    "7. Extraction des hashtags bruts des tweets clients → colonne `hashtags_list`.\n",
    "8. Nettoyage du texte client pour le LLM → colonne `text_clean_llm`.\n",
    "9. Contrôle des hashtags perdus → colonne `lost_hashtags_count`.\n",
    "10. Déduplication sur `id` (un tweet client unique par id).\n",
    "11. Calcul des statistiques globales (`rows_in`, `rows_out`, `retweets_free`, `retweets_total`, `reste`, `dedup_removed`, `lost_hashtags_total`).\n",
    "\n",
    "La fonction renvoie :\n",
    "- `df` : DataFrame nettoyé (tweets clients),\n",
    "- `stats` : dictionnaire de statistiques de nettoyage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1825bee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Pipeline complet de nettoyage des tweets.\n",
    "    \"\"\"\n",
    "    orig_len = len(df)\n",
    "\n",
    "    # Normalisation des noms de colonnes (avec gestion BOM)\n",
    "    df = df.copy()\n",
    "    df.columns = [_normalize_col_name(c) for c in df.columns]\n",
    "\n",
    "    # Vérification de la colonne 'id'\n",
    "    if \"id\" not in df.columns:\n",
    "        raise SystemExit(\n",
    "            \"La colonne 'id' est obligatoire pour dédupliquer les tweets.\\n\"\n",
    "            f\"Colonnes disponibles après normalisation : {', '.join(df.columns)}\"\n",
    "        )\n",
    "\n",
    "    # Détection de la colonne texte\n",
    "    text_col = next((c for c in [\"full_text\", \"text\", \"tweet\", \"content\"] if c in df.columns), None)\n",
    "    if not text_col:\n",
    "        raise SystemExit(\"Colonne texte introuvable (full_text / text / tweet / content).\")\n",
    "\n",
    "    # Texte brut -> text_raw\n",
    "    df[\"text_raw\"] = df[text_col].astype(str)\n",
    "\n",
    "    # Normaliser UNE FOIS text_raw (il devient notre texte de référence \"propre\")\n",
    "    df[\"text_raw\"] = df[\"text_raw\"].apply(normalize_whitespace_tokens)\n",
    "\n",
    "    # Détection des retweets sur TOUT le dataset (avant filtre Free)\n",
    "    has_retweeted_status = \"retweeted_status\" in df.columns\n",
    "    if has_retweeted_status:\n",
    "        retweeted_status_notna = df[\"retweeted_status\"].notna()\n",
    "    else:\n",
    "        retweeted_status_notna = False\n",
    "\n",
    "    df[\"is_retweet\"] = df[\"text_raw\"].str.match(RT_PAT, na=False) | retweeted_status_notna\n",
    "\n",
    "    # Filtre des comptes Free (annonces / réponses / retweets de Free)\n",
    "    POSSIBLE_HANDLE_COLS = [\n",
    "        \"screen_name\", \"user_screen_name\", \"username\",\n",
    "        \"user\", \"author\", \"account\", \"handle\"\n",
    "    ]\n",
    "    handle_col = next((c for c in POSSIBLE_HANDLE_COLS if c in df.columns), None)\n",
    "    removed = 0\n",
    "    retweets_free = 0  # retweets dans les lignes filtrées (Free)\n",
    "\n",
    "    if handle_col:\n",
    "        df[\"__handle_norm\"] = (\n",
    "            df[handle_col].astype(str)\n",
    "            .str.replace(r\"^@\", \"\", regex=True)\n",
    "            .str.strip()\n",
    "            .str.casefold()\n",
    "        )\n",
    "        BLOCKED = {\"free\", \"free_1337\", \"freebox\", \"freefoot\", \"freenewsactu\", \"universfreebox\"}\n",
    "        mask_free = df[\"__handle_norm\"].isin(BLOCKED)\n",
    "\n",
    "        removed = int(mask_free.sum())\n",
    "        # Retweets parmi les tweets Free (avant de les enlever)\n",
    "        retweets_free = int(df.loc[mask_free, \"is_retweet\"].sum())\n",
    "\n",
    "        # On enlève les lignes Free pour ne garder que les tweets client\n",
    "        df = df[~mask_free].drop(columns=[\"__handle_norm\"])\n",
    "    else:\n",
    "        mask_free = None  # cohérence\n",
    "\n",
    "    # Hashtags bruts sur les tweets clients (à partir de text_raw déjà normalisé)\n",
    "    df[\"hashtags_list\"] = df[\"text_raw\"].apply(extract_hashtags)\n",
    "\n",
    "    # Texte nettoyé LLM à partir de text_raw (déjà normalisé)\n",
    "    df[\"text_clean_llm\"] = df[\"text_raw\"].apply(clean_llm_text)\n",
    "\n",
    "    # Contrôle des hashtags perdus\n",
    "    def _lost_hashtags_count_row(row):\n",
    "        raw_list = row.get(\"hashtags_list\")\n",
    "        if not isinstance(raw_list, list):\n",
    "            raw_list = []\n",
    "        rset = {t.casefold() for t in raw_list}\n",
    "        cset = {t.casefold() for t in extract_hashtags(row.get(\"text_clean_llm\", \"\"))}\n",
    "        return max(0, len(rset - cset))\n",
    "\n",
    "    df[\"lost_hashtags_count\"] = df.apply(_lost_hashtags_count_row, axis=1)\n",
    "\n",
    "    # Déduplication uniquement sur id\n",
    "    before = len(df)\n",
    "    df[\"id\"] = df[\"id\"].astype(str).str.strip()\n",
    "    df = df.dropna(subset=[\"id\"]).drop_duplicates(subset=[\"id\"], keep=\"first\")\n",
    "    dedup_removed = before - len(df)\n",
    "\n",
    "    # Stats\n",
    "    rows_out = int(len(df))\n",
    "    retweets_client = int(df[\"is_retweet\"].sum())  # côté clients (non affiché)\n",
    "    retweets_total = retweets_client + retweets_free\n",
    "    reste = int(removed)  # tweets Free filtrés\n",
    "\n",
    "    stats = {\n",
    "        \"rows_in\": int(orig_len),\n",
    "        \"rows_out\": rows_out,\n",
    "        \"tweets_client\": rows_out,\n",
    "        \"retweets_free\": retweets_free,\n",
    "        \"retweets_total\": retweets_total,\n",
    "        \"reste\": reste,\n",
    "        \"dedup_removed\": int(dedup_removed),\n",
    "        \"filtered_handles\": int(removed),\n",
    "        \"lost_hashtags_total\": int(df[\"lost_hashtags_count\"].sum()),\n",
    "    }\n",
    "    return df, stats\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
